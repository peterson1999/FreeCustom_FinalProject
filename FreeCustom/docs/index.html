<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>FreeCustom</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <!--  support TeX in .html file  -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      jax: ["input/TeX", "output/HTML-CSS"],
      extensions: ["tex2jax.js"],
      "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] },
      tex2jax: { inlineMath: [ ["$", "$"], ["\\(","\\)"] ], displayMath: [ ["$$","$$"], ["\\[", "\\]"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
      TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
      messageStyle: "none"
    });
    </script>    
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js"></script>
  <!--  support TeX in .html file  -->

</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">FreeCustom: Tuning-Free Customized Image Generation for Multi-Concept Composition</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://dingangui.github.io">Ganggui Ding</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://volcverse.vercel.app">Canyu Zhao</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://github.com/encounter1997">Wen Wang</a><sup>*</sup>,
            </span>
            <span class="author-block">
              <a href="https://zhenyangcs.github.io/">Zhen Yang</a><sup></sup>,
            </span>
            <span class="author-block">
              <a href="https://github.com/zideliu">Zide Liu</a><sup></sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=FaOqRpcAAAAJ">Hao Chen</a><sup>†</sup>,
            </span>
			      <span class="author-block">
              <a href="https://cshen.github.io/">Chunhua Shen</a><sup>†</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
              <span class="author-block"><sup></sup>CAD&CG, Zhejiang University, China</span>
          </div>

          <span class=""><sup>*</sup>Equal Contribution</span>
          <span class=""><sup>†</sup>Corrsponding Author</span>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2405.13870"
                    class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/aim-uofa/FreeCustom"
                    class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Begin Teaser -->
<div>
  <div class="columns is-centered has-text-centered">
    <h2 class="title is-3">Customized Image Generation</h2>
  </div>
  </br>
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div> 
        <img src="./static/images/results_of_multi_concept.png"
        class="interpolation-image"
        alt="Interpolate start reference image."/>
        
        <img src="./static/images/results_of_single_concept.png"
        class="interpolation-image"
        alt="Interpolate start reference image."/>
        </div>
      <div class="hero-body">
        <h2 class="subtitle has-text-justified">
          <p>Our method excels at <i>rapidly</i> generating high-quality images with multiple concept combinations and single concept customization, without any model parameter tuning. The identity of each concept is remarkably preserved. Furthermore, our method exhibits great versatility and robustness when dealing with different categories of concepts. This versatility allows users to generate customized images that involve diverse combinations of concepts, catering to their specific needs and preferences.</p>
        </h2>
      </div>
    </div>
  </section>
</div>
<!-- End Teaser -->


<!-- Begin abstract -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <!-- <img src='static/images/method_overview.png' id="framework"><br><br> -->
        <div class="content has-text-justified">
          <p>
            Benefiting from large-scale pre-trained text-to-image (T2I) generative models, impressive progress has been achieved in customized image generation, which aims to generate user-specified concepts.
            Existing approaches have extensively focused on single-concept customization and still encounter challenges when it comes to complex scenarios that involve combining multiple concepts. These approaches often require retraining/fine-tuning using a few images, leading to time-consuming training processes and impeding their swift implementation. 
            Furthermore, the reliance on multiple images to represent a singular concept increases the difficulty of customization.
          </p>
          <p>
            To this end, we propose <b>FreeCustom</b>, a novel tuning-free method to generate customized images of multi-concept composition based on reference concepts, using only one image per concept as input. Specifically, we introduce a new multi-reference self-attention (MRSA) mechanism and a weighted mask strategy that enables the generated image to access and focus more on the reference concepts. In addition, MRSA leverages our key finding that input concepts are better preserved when providing images with context interactions.
            Experiments show that our method's produced images are consistent with the given concepts and better aligned with the input text.
            Our method outperforms or performs on par with other training-based methods in terms of multi-concept composition and single-concept customization, but is simpler.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End abstract -->


<!-- Begin method overview -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Method Overview. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method Overview</h2>
        <img src='static/images/method_overview.png' id="framework"><br>
        <div class="content has-text-justified">
          <p>
            Given a set of reference images $ \mathcal{I} = \{I_1, I_2, I_3\} $ and their corresponding prompts $\mathcal{P} = \{P_1, P_2, P_3\}$, we generate a multi-concept customized composition image $I$ aligned to the target prompt $P$. (a) We use a VAE encoder to convert reference images into the latent representation $\mathbf z_0'$ and a segmentation network to extract masks of the concepts. (b) The denoising process involves two paths: <i>1)</i> the concepts reference path and <i>2)</i> the concepts composition path. In <i>1)</i>, we employ a diffusion forward process to transform $\mathbf z_0'$ into $\mathbf z_t'$, subsequently passing $\mathbf z_t'$ to the U-Net $\epsilon_\theta$. Notably, the output of $\epsilon_\theta$ isn't used. In <i>2)</i>, we initially sample $\mathbf z_T \sim \mathcal{N} (0,\textbf{I})$ and iteratively denoise the latent until we obtain $\mathbf z_0$. At each time step $t$, we directly transmit the current latent $\mathbf z_t$ to the modified U-Net $\epsilon_\theta^*$ and employ the MRSA to integrate the features from the last two blocks of both the U-Net $\epsilon_\theta$ and the U-Net $\epsilon_\theta^*$. Finally, we utilize a VAE decoder to convert $\mathbf{z_0}$ into the final image $I$. (c) The MRSA mechanism. <i>i)</i> Feature injection happens in the self-attention module between U-Net layers, <i>ii)</i> we apply MRSA machenism using $$ {\rm MRSA} (\mathbf{Q}, \mathbf{K}', \mathbf{V}',\mathbf{M}_w) = {\rm Softmax} (\frac{\mathbf{M}_w \odot (\mathbf{Q} \mathbf{K}'^T)}{\sqrt{d}}) \mathbf{V}'.$$
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End method overview -->

<!-- Begin Paradigm Comparison -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Method Overview. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Paradigm Comparison</h2>
        <img src='static/images/paradigm_comparison.png' id="framework"><br>
        <div class="content has-text-justified">
          <p>
            Previous methods for customization can be categorized into two main categories: (a) training-based methods and (b) tailored models for generalizable customization. Training-based methods often involve fine-tuning an entire model (Type Ⅰ) or learning a text embedding to represent a specific subject (Type Ⅱ). Tailored models typically require re-training on large-scale image datasets to establish a versatile foundation. Unlike these two types of methods, our approach can directly generate customized images of multi-concept combinations without any additional training.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Paradigm Comparison -->

<!-- Begin Qualitative Comparison -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Method Overview. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Qualitative Comparison</h2>
        <img src='static/images/comparison_with_other_methods_multi.png' id="framework">
        <div class="content has-text-centered">
          <p>
            Comparisons of multi-concept composition.
          </p>
        </div>
        <img src='static/images/comparison_with_other_methods_single.png' id="framework">
        <div class="content has-text-centered">
          <p>
            Comparisons of single-concept customization.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Qualitative Comparison -->


<!-- Begin More Visual Results. -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Method Overview. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">More Visual Results</h2>
        <img src='static/images/cat1_qualitative_results.png' id="framework">
        <div class="content has-text-centered">
          <p>
            <b>Single-concept customization.</b> Our method enables extensive customization of a single concept by inputting a single image
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End More Visual Results. -->


<!-- Begin Appearance Transfer. -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Method Overview. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Appearance Transfer</h2>
        <img src='static/images/appearance transfer.png' id="framework">
        <div class="content has-text-centered">
          <p>
            Our method generates objects with similar appearance and materials as the input image.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Appearance Transfer. -->


<!-- Begin Empower other methods. -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Method Overview. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Empower Other Methods</h2>
        <img src='static/images/blipdiffusion with ours.png' id="framework">
        <div class="content has-text-centered">
          <p>
            (a) BLIP Diffusion with Ours.
          </p>
        </div>
        <img src='static/images/ControlNet with ours.png' id="framework">
        <div class="content has-text-centered">
          <p>
            (b) ControlNet with Ours.
          </p>
        </div>
        <div class="content has-text-justified">
          <p>
            Our method can enhance ControlNet and BLIP Diffusion in a plug-and-play manner. (a) By using our method, the output of BLIP diffusion becomes more faithful to the input image and better aligned with the input text. (b) Furthermore, ControlNet can generate results that are consistent in layout and identity when combined with ours.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Empower other methods. -->
<br><br>
<!-- End More Visual Results.-->


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{ding2024freecustom,
  title={FreeCustom: Tuning-Free Customized Image Generation for Multi-Concept Composition}, 
  author={Ganggui Ding and Canyu Zhao and Wen Wang and Zhen Yang and Zide Liu and Hao Chen and Chunhua Shen},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2024}
}
    </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="content has-text-centered">
        <div class="content">
          <p>
            This website was modified from <a
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. We appreciate for sharing this perfect template!
          </p>
        </div>
    </div>
  </div>
</footer>

</body>
</html>
 